<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="shortcut icon"
href="/assets/ico/favicon.ico"> -->

    <title>Probabilistic Numerics.org</title>

    <!-- Bootstrap core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="/assets/css/jwvdm.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="/assets/js/html5shiv.js"></script>
      <script src="/assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="container">
      <div class="header row">
        <div class="row">
  
  <a href="http://www.probabilistic-numerics.org"> 
    <img class="logo" src="/assets/images/Graphs.png" height="80px" align="bottom"> 
  </a>
</div>
<div class="row">
    <div class="title pull-left">
      Probabilistic-Numerics.org
    </div>
    <ul class="nav nav-pills pull-right">
      
      
      <li >
        <a href="/index.html" title="">About/News</a>
      </li>
      <!-- class="active"><a href="/">About</a></li> -->
      <!-- <li><a href="/projects">Projects</a></li> -->
      <!-- <li><a href="/publications">Publications</a></li> -->
      
      
      <li >
        <a href="/research/index.html" title="">Research Areas</a>
      </li>
      <!-- class="active"><a href="/">About</a></li> -->
      <!-- <li><a href="/projects">Projects</a></li> -->
      <!-- <li><a href="/publications">Publications</a></li> -->
      
      
      <li class="active">
        <a href="/literature/index.html" title="">Literature</a>
      </li>
      <!-- class="active"><a href="/">About</a></li> -->
      <!-- <li><a href="/projects">Projects</a></li> -->
      <!-- <li><a href="/publications">Publications</a></li> -->
      
    </ul>
</div>

<script>
  $()
</script>
       
      </div>
      <div class="content row">
        <h1>Literature</h1>
<p>This page collects literature on all areas of probabilistic
numerics, sorted by problem type. If you would like your publication to be
featured in this list, please contact us by email.</p>

<h3 id="quick-jump-links">Quick-jump links:</h3>

<ul>
  <li><a href="#General">General and Foundational</a></li>
  <li><a href="#Quadrature">Quadrature</a></li>
  <li><a href="#Linear">Linear Algebra</a></li>
  <li><a href="#Optimization">Optimization</a></li>
  <li><a href="#ODEs">Ordinary Differential Equations</a></li>
  <li><a href="#PDEs">Partial Differential Equations</a></li>
</ul>

<h2 id="General">General and Foundational</h2>
<p>The following papers are often cited as early works on the
idea of uncertainty over the result of deterministic computations.</p>

<ol class="bibliography"><li><span id="kac1940gaussian">Erdös, P., &amp; Kac, M. (1940). The Gaussian law of errors in the theory of additive number
                  theoretic functions. <i>American Journal of Mathematics</i>, 738–742.</span>

<span id="kac1940gaussian_materials">
  <ul class="nav nav-pills">
    

    <li><a class="bib-materials" data-target="#kac1940gaussian_bibtex" data-toggle="collapse" href="#kac1940gaussian" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://www.jstor.org/stable/2371483">web</a></li>
    

    
  </ul>

  

  <pre id="kac1940gaussian_bibtex" class="pre pre-scrollable collapse">@article{kac1940gaussian,
  title = {The Gaussian law of errors in the theory of additive number
                    theoretic functions},
  author = {Erd{\"o}s, P. and Kac, M},
  journal = {American Journal of Mathematics},
  pages = {738--742},
  year = {1940},
  link = {http://www.jstor.org/stable/2371483}
}
</pre>

</span>
</li>
<li><span id="kac1949distributions">Kac, M. (1949). On distributions of certain Wiener functionals. <i>Transactions of the AMS</i>, <i>65</i>(1), 1–13.</span>

<span id="kac1949distributions_materials">
  <ul class="nav nav-pills">
    

    <li><a class="bib-materials" data-target="#kac1949distributions_bibtex" data-toggle="collapse" href="#kac1949distributions" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://www.jstor.org/stable/1990512">web</a></li>
    

    
  </ul>

  

  <pre id="kac1949distributions_bibtex" class="pre pre-scrollable collapse">@article{kac1949distributions,
  title = {On distributions of certain Wiener functionals},
  author = {Kac, M.},
  journal = {Transactions of the AMS},
  volume = {65},
  number = {1},
  pages = {1--13},
  year = {1949},
  link = {http://www.jstor.org/stable/1990512}
}
</pre>

</span>
</li>
<li><span id="diaconis1988bayesian">Diaconis, P. (1988). Bayesian numerical analysis. <i>Statistical Decision Theory and Related Topics IV</i>, <i>1</i>, 163–175.</span>

<span id="diaconis1988bayesian_materials">
  <ul class="nav nav-pills">
    

    <li><a class="bib-materials" data-target="#diaconis1988bayesian_bibtex" data-toggle="collapse" href="#diaconis1988bayesian" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Diaconis_1988.pdf">PDF</a></li>
    
    
    

    
  </ul>

  

  <pre id="diaconis1988bayesian_bibtex" class="pre pre-scrollable collapse">@article{diaconis1988bayesian,
  title = {Bayesian numerical analysis},
  author = {Diaconis, Persi},
  journal = {Statistical decision theory and related topics IV},
  volume = {1},
  pages = {163--175},
  year = {1988},
  publisher = {Springer-Verlag, New York},
  file = {../assets/pdf/Diaconis_1988.pdf}
}
</pre>

</span>
</li></ol>

<h2 id="Quadrature">Quadrature</h2>

<ol class="bibliography"><li><span id="o1991bayes">O’Hagan, A. (1991). Bayes–Hermite quadrature. <i>Journal of Statistical Planning and Inference</i>, <i>29</i>(3), 245–260.</span>

<span id="o1991bayes_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#o1991bayes_abstract" data-toggle="collapse" href="#o1991bayes" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#o1991bayes_bibtex" data-toggle="collapse" href="#o1991bayes" onclick="return false">Bib</a></li>

    
    
    

    
  </ul>

  
  <p id="o1991bayes_abstract" class="collapse">Bayesian quadrature treats the problem of numerical
                  integration as one of statistical inference. A prior Gaussian
                  process distribution is assumed for the integrand,
                  observations arise from evaluating the integrand at selected
                  points, and a posterior distribution is derived for the
                  integrand and the integral. Methods are developed for
                  quadrature in p. A particular application is integrating the
                  posterior density arising from some other Bayesian analysis.
                  Simulation results are presented, to show that the resulting
                  Bayes–Hermite quadrature rules may perform better than the
                  conventional Gauss–Hermite rules for this application. A key
                  result is derived for product designs, which makes Bayesian
                  quadrature practically useful for integrating in several
                  dimensions. Although the method does not at present provide a
                  solution to the more difficult problem of quadrature in high
                  dimensions, it does seem to offer real improvements over
                  existing methods in relatively low dimensions.</p>
  

  <pre id="o1991bayes_bibtex" class="pre pre-scrollable collapse">@article{o1991bayes,
  title = {{B}ayes--{H}ermite quadrature},
  author = {O'Hagan, A.},
  journal = {Journal of statistical planning and inference},
  volume = {29},
  number = {3},
  pages = {245--260},
  year = {1991}
}
</pre>

</span>
</li>
<li><span id="minka2000deriving">Minka, T. P. (2000). <i>Deriving quadrature rules from Gaussian processes</i>. Statistics Department, Carnegie Mellon University.</span>

<span id="minka2000deriving_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#minka2000deriving_abstract" data-toggle="collapse" href="#minka2000deriving" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#minka2000deriving_bibtex" data-toggle="collapse" href="#minka2000deriving" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://research.microsoft.com/en-us/um/people/minka/papers/quadrature.html">web</a></li>
    

    
  </ul>

  
  <p id="minka2000deriving_abstract" class="collapse">Quadrature rules are often designed to achieve zero error on
                  a small set of functions, e.g. polynomials of specified
                  degree. A more robust method is to minimize average error
                  over a large class or distribution of functions. If functions
                  are distributed according to a Gaussian process, then
                  designing an average-case quadrature rule reduces to solving
                  a system of 2n equations, where n is the number of nodes in
                  the rule (O’Hagan, 1991). It is shown how this very general
                  technique can be used to design customized quadrature rules,
                  in the style of Yarvin &amp; Rokhlin (1998), without the need for
                  singular value decomposition and in any number of
                  dimensions. It is also shown how classical Gaussian
                  quadrature rules, trigonometric lattice rules, and spline
                  rules can be extended to the average-case and to multiple
                  dimensions by deriving them from Gaussian processes. In
                  addition to being more robust, multidimensional quadrature
                  rules designed for the average-case are found to be much less
                  ambiguous than those designed for a given polynomial degree.</p>
  

  <pre id="minka2000deriving_bibtex" class="pre pre-scrollable collapse">@techreport{minka2000deriving,
  author = {Minka, T.P.},
  institution = {Statistics Department, Carnegie Mellon University},
  title = {{Deriving quadrature rules from {G}aussian processes}},
  year = {2000},
  link = {http://research.microsoft.com/en-us/um/people/minka/papers/quadrature.html}
}
</pre>

</span>
</li>
<li><span id="osborne2012bayesian">Osborne, M. A., Garnett, R., Roberts, S. J., Hart, C., Aigrain, S., &amp; Gibson, N. (2012). Bayesian quadrature for ratios. In <i>International Conference on Artificial Intelligence and
                  Statistics</i> (pp. 832–840).</span>

<span id="osborne2012bayesian_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#osborne2012bayesian_abstract" data-toggle="collapse" href="#osborne2012bayesian" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#osborne2012bayesian_bibtex" data-toggle="collapse" href="#osborne2012bayesian" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Osborne2012Bayesian.pdf">PDF</a></li>
    
    
    

    
  </ul>

  
  <p id="osborne2012bayesian_abstract" class="collapse">We describe a novel approach to quadrature for ratios of
                  probabilistic integrals, such as are used to compute
                  posterior probabilities. This approach offers performance
                  superior to Monte Carlo methods by exploiting a Bayesian
                  quadrature framework. We improve upon previous Bayesian
                  quadrature techniques by explicitly modelling the
                  non-negativity of our integrands, and the correlations that
                  exist between them. It offers most where the integrand is
                  multi-modal and expensive to evaluate. We demonstrate the
                  efficacy of our method on data from the Kepler space
                  telescope.</p>
  

  <pre id="osborne2012bayesian_bibtex" class="pre pre-scrollable collapse">@inproceedings{osborne2012bayesian,
  author = {Osborne, M.A. and Garnett, R. and Roberts, S.J. and Hart, C. and Aigrain, S. and Gibson, N.},
  booktitle = {{International Conference on Artificial Intelligence and
                    Statistics}},
  pages = {832--840},
  title = {{Bayesian quadrature for ratios}},
  year = {2012},
  file = {../assets/pdf/Osborne2012Bayesian.pdf}
}
</pre>

</span>
</li>
<li><span id="osborne2012active">Osborne, M. A., Duvenaud, D. K., Garnett, R., Rasmussen, C. E., Roberts, S. J., &amp; Ghahramani, Z. (2012). Active Learning of Model Evidence Using Bayesian
                  Quadrature. In <i>Advances in Neural Information Processing Systems (NIPS)</i> (pp. 46–54).</span>

<span id="osborne2012active_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#osborne2012active_abstract" data-toggle="collapse" href="#osborne2012active" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#osborne2012active_bibtex" data-toggle="collapse" href="#osborne2012active" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Osborne2012active.pdf">PDF</a></li>
    
    
    

    
  </ul>

  
  <p id="osborne2012active_abstract" class="collapse"> Numerical integration is a key component of many problems in
                  scientific computing, statistical modelling, and machine
                  learning. Bayesian Quadrature is a model-based method for
                  numerical integration which, relative to standard Monte Carlo
                  methods, offers increased sample efficiency and a more robust
                  estimate of the uncertainty in the estimated integral. We
                  propose a novel Bayesian Quadrature approach for numerical
                  integration when the integrand is non-negative, such as the
                  case of computing the marginal likelihood, predictive
                  distribution, or normalising constant of a probabilistic
                  model. Our approach approximately marginalises the quadrature
                  model’s hyperparameters in closed form, and introduces an ac-
                  tive learning scheme to optimally select function
                  evaluations, as opposed to using Monte Carlo samples. We
                  demonstrate our method on both a number of synthetic
                  benchmarks and a real scientific problem from astronomy.</p>
  

  <pre id="osborne2012active_bibtex" class="pre pre-scrollable collapse">@inproceedings{osborne2012active,
  author = {Osborne, M.A. and Duvenaud, D.K. and Garnett, R. and Rasmussen, C.E. and Roberts, S.J. and Ghahramani, Z.},
  booktitle = {{Advances in Neural Information Processing Systems (NIPS)}},
  pages = {46--54},
  title = {{Active Learning of Model Evidence Using Bayesian
                    Quadrature.}},
  year = {2012},
  file = {../assets/pdf/Osborne2012active.pdf}
}
</pre>

</span>
</li></ol>

<h2 id="Linear">Linear Algebra</h2>

<ol class="bibliography"><li><span id="2014arXiv14022058H">Hennig, P. (2014). Probabilistic Interpretation of Linear Solvers. <i>ArXiv Preprint 1402.2058</i>.</span>

<span id="2014arXiv14022058H_materials">
  <ul class="nav nav-pills">
    

    <li><a class="bib-materials" data-target="#2014arXiv14022058H_bibtex" data-toggle="collapse" href="#2014arXiv14022058H" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://arxiv.org/abs/1402.2058">web</a></li>
    

    
  </ul>

  

  <pre id="2014arXiv14022058H_bibtex" class="pre pre-scrollable collapse">@article{2014arXiv14022058H,
  author = {{Hennig}, P.},
  journal = {arXiv preprint 1402.2058},
  month = {feb},
  title = {{Probabilistic Interpretation of Linear Solvers}},
  year = {2014},
  link = {http://arxiv.org/abs/1402.2058}
}
</pre>

</span>
</li></ol>

<h2 id="Optimization">Optimization</h2>

<ol class="bibliography"><li><span id="HennigKiefel">Hennig, P., &amp; Kiefel, M. (2012). Quasi-Newton methods – a new direction. In <i>International Conference on Machine Learning (ICML)</i>.</span>

<span id="HennigKiefel_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#HennigKiefel_abstract" data-toggle="collapse" href="#HennigKiefel" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#HennigKiefel_bibtex" data-toggle="collapse" href="#HennigKiefel" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Hennig13quasiNewton.pdf">PDF</a></li>
    
    
    

    
    <li><a class="bib-materials" href="http://techtalks.tv/talks/quasi-newton-methods-a-new-direction/57289/">video</a></li>
    
  </ul>

  
  <p id="HennigKiefel_abstract" class="collapse">Four decades after their invention, quasi- Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Al- though not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of clas- sical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.</p>
  

  <pre id="HennigKiefel_bibtex" class="pre pre-scrollable collapse">@inproceedings{HennigKiefel,
  author = {Hennig, P. and Kiefel, M.},
  booktitle = {{International Conference on Machine Learning (ICML)}},
  title = {{Quasi-{N}ewton methods -- a new direction}},
  year = {2012},
  video = {http://techtalks.tv/talks/quasi-newton-methods-a-new-direction/57289/},
  file = {../assets/pdf/Hennig13quasiNewton.pdf}
}
</pre>

</span>
</li>
<li><span id="hennig13_quasi_newton_method">Hennig, P., &amp; Kiefel, M. (2013). Quasi-Newton Methods – a new direction. <i>Journal of Machine Learning Research</i>, <i>14</i>, 834–865.</span>

<span id="hennig13_quasi_newton_method_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#hennig13_quasi_newton_method_abstract" data-toggle="collapse" href="#hennig13_quasi_newton_method" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#hennig13_quasi_newton_method_bibtex" data-toggle="collapse" href="#hennig13_quasi_newton_method" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Hennig13qN_JMLR.pdf">PDF</a></li>
    
    
    

    
  </ul>

  
  <p id="hennig13_quasi_newton_method_abstract" class="collapse">Four decades after their invention, quasi-Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Although not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of classical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.</p>
  

  <pre id="hennig13_quasi_newton_method_bibtex" class="pre pre-scrollable collapse">@article{hennig13_quasi_newton_method,
  author = {Hennig, P. and Kiefel, M.},
  journal = {Journal of Machine Learning Research},
  month = {mar},
  pages = {834--865},
  title = {Quasi-{N}ewton Methods -- a new direction},
  volume = {14},
  year = {2013},
  file = {../assets/pdf/Hennig13qN_JMLR.pdf}
}
</pre>

</span>
</li>
<li><span id="StochasticNewton">Hennig, P. (2013). Fast Probabilistic Optimization from Noisy Gradients. In <i>International Conference on Machine Learning (ICML)</i>.</span>

<span id="StochasticNewton_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#StochasticNewton_abstract" data-toggle="collapse" href="#StochasticNewton" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#StochasticNewton_bibtex" data-toggle="collapse" href="#StochasticNewton" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Hennig13noisy.pdf">PDF</a></li>
    
    
    

    
  </ul>

  
  <p id="StochasticNewton_abstract" class="collapse">Stochastic gradient descent remains popular in large-scale
                  machine learning, on account of its very low computational
                  cost and robust- ness to noise. However, gradient descent is
                  only linearly efficient and not transformation
                  invariant. Scaling by a local measure can substantially
                  improve its performance. One natural choice of such a scale
                  is the Hessian of the objective function: Were it available,
                  it would turn linearly efficient gradient descent into the
                  quadratically efficient Newton-Raphson optimization. Existing
                  covariant methods, though, are either super-linearly
                  expensive or do not address noise. Generalising recent
                  results, this paper constructs a nonparametric Bayesian
                  quasi-Newton algorithm that learns gradient and Hessian from
                  noisy evaluations of the gradient. Importantly, the resulting
                  algorithm, like stochastic gradient descent, has cost linear
                  in the number of input dimensions</p>
  

  <pre id="StochasticNewton_bibtex" class="pre pre-scrollable collapse">@inproceedings{StochasticNewton,
  author = {Hennig, P.},
  booktitle = {{International Conference on Machine Learning (ICML)}},
  title = {{Fast Probabilistic Optimization from Noisy Gradients}},
  year = {2013},
  file = {../assets/pdf/Hennig13noisy.pdf}
}
</pre>

</span>
</li></ol>

<h2 id="ODEs">Ordinary Differential Equations</h2>

<ol class="bibliography"><li><span id="skilling1991bayesian">Skilling, J. (1991). Bayesian solution of ordinary differential equations. <i>Maximum Entropy and Bayesian Methods, Seattle</i>.</span>

<span id="skilling1991bayesian_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#skilling1991bayesian_abstract" data-toggle="collapse" href="#skilling1991bayesian" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#skilling1991bayesian_bibtex" data-toggle="collapse" href="#skilling1991bayesian" onclick="return false">Bib</a></li>

    
    
    

    
  </ul>

  
  <p id="skilling1991bayesian_abstract" class="collapse">In the numerical solution of ordinary differential equations,
                  a function y(x) is to be reconstructed from knowledge of the
                  functional form of its derivative: dy/dx=f(x,y), together
                  with an appropriate boundary condition. The derivative f is
                  evaluated at a sequence of suitably chosen points (x_k,y_k),
                  from which the form of y(.) is estimated. This is an
                  inference problem, which can and perhaps should be treated by
                  Bayesian techniques. As always, the inference appears as a
                  probability distribution prob(y(.)), from which random
                  samples show the probabilistic reliability of the
                  results. Examples are given.</p>
  

  <pre id="skilling1991bayesian_bibtex" class="pre pre-scrollable collapse">@article{skilling1991bayesian,
  author = {Skilling, J.},
  journal = {Maximum Entropy and Bayesian Methods, Seattle},
  title = {{Bayesian solution of ordinary differential equations}},
  year = {1991}
}
</pre>

</span>
</li>
<li><span id="HennigAISTATS2014">Hennig, P., &amp; Hauberg, S. (2014). Probabilistic Solutions to Differential Equations and their
                  Application to Riemannian Statistics. In <i>Proc. of the 17th int. Conf. on Artificial Intelligence and
                  Statistics (AISTATS)</i> (Vol. 33). JMLR, W&amp;CP.</span>

<span id="HennigAISTATS2014_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#HennigAISTATS2014_abstract" data-toggle="collapse" href="#HennigAISTATS2014" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#HennigAISTATS2014_bibtex" data-toggle="collapse" href="#HennigAISTATS2014" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="http://jmlr.org/proceedings/papers/v33/hennig14.pdf">PDF</a></li>
    
    
    

    
    <li><a class="bib-materials" href="https://www.youtube.com/watch?v=fLCS0KXmiXs">video</a></li>
    
  </ul>

  
  <p id="HennigAISTATS2014_abstract" class="collapse">We study a probabilistic numerical method for the solution of
                  both boundary and initial value problems that returns a joint
                  Gaussian process posterior over the solution. Such methods
                  have concrete value in the statistics on Riemannian
                  manifolds, where non-analytic ordinary differential equations
                  are involved in virtually all computations. The probabilistic
                  formulation permits marginalising the uncertainty of the
                  numerical solution such that statistics are less sensitive to
                  inaccuracies. This leads to new Riemannian algorithms for
                  mean value computations and principal geodesic
                  analysis. Marginalisation also means results can be less
                  precise than point estimates, enabling a noticeable speed-up
                  over the state of the art. Our approach is an argument for a
                  wider point that uncertainty caused by numerical calculations
                  should be tracked throughout the pipeline of machine learning
                  algorithms.</p>
  

  <pre id="HennigAISTATS2014_bibtex" class="pre pre-scrollable collapse">@inproceedings{HennigAISTATS2014,
  author = {Hennig, Philipp and Hauberg, S{\o}ren},
  booktitle = {{Proc. of the 17th int. Conf. on Artificial Intelligence and
                    Statistics ({AISTATS})}},
  publisher = {JMLR, W\&amp;CP},
  title = {{Probabilistic Solutions to Differential Equations and their
                    Application to Riemannian Statistics}},
  volume = {33},
  year = {2014},
  file = {http://jmlr.org/proceedings/papers/v33/hennig14.pdf},
  video = {https://www.youtube.com/watch?v=fLCS0KXmiXs}
}
</pre>

</span>
</li>
<li><span id="13_bayes_uncer_quant_differ_equat">Chkrebtii, O., Campbell, D. A., Girolami, M. A., &amp; Calderhead, B. (2013). Bayesian Uncertainty Quantification for Differential
                  Equations. <i>ArXiv PrePrint 1306.2365</i>.</span>

<span id="13_bayes_uncer_quant_differ_equat_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#13_bayes_uncer_quant_differ_equat_abstract" data-toggle="collapse" href="#13_bayes_uncer_quant_differ_equat" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#13_bayes_uncer_quant_differ_equat_bibtex" data-toggle="collapse" href="#13_bayes_uncer_quant_differ_equat" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://arxiv.org/abs/1306.2365">web</a></li>
    

    
  </ul>

  
  <p id="13_bayes_uncer_quant_differ_equat_abstract" class="collapse">This paper advocates expansion of the role of Bayesian
                  statistical inference when formally quantifying uncertainty
                  in computer models defined by systems of ordinary or partial
                  differential equations. We adopt the perspective that
                  implicitly defined infinite dimensional functions
                  representing model states are objects to be inferred
                  probabilistically. We develop a general methodology for the
                  probabilistic integration of differential equations via model
                  based updating of a joint prior measure on the space of
                  functions and their temporal and spatial derivatives. This
                  results in a posterior measure over functions reflecting how
                  well they satisfy the system of differential equations and
                  corresponding initial and boundary values. We show how this
                  posterior measure can be naturally incorporated within the
                  Kennedy and O’Hagan framework for uncertainty quantification
                  and provides a fully Bayesian approach to model
                  calibration. By taking this probabilistic viewpoint, the full
                  force of Bayesian inference can be exploited when seeking to
                  coherently quantify and propagate epistemic uncertainty in
                  computer models of complex natural and physical systems. A
                  broad variety of examples are provided to illustrate the
                  potential of this framework for characterising discretization
                  uncertainty, including initial value, delay, and boundary
                  value differential equations, as well as partial differential
                  equations. We also demonstrate our methodology on a large
                  scale system, by modeling discretization uncertainty in the
                  solution of the Navier-Stokes equations of fluid flow,
                  reduced to over 16,000 coupled and stiff ordinary
                  differential equations. Finally, we discuss the wide range of
                  open research themes that follow from the work presented.</p>
  

  <pre id="13_bayes_uncer_quant_differ_equat_bibtex" class="pre pre-scrollable collapse">@article{13_bayes_uncer_quant_differ_equat,
  author = {Chkrebtii, O. and Campbell, D.A. and Girolami, M.A. and Calderhead, B.},
  journal = {arXiv prePrint 1306.2365},
  title = {{B}ayesian Uncertainty Quantification for Differential
                    Equations},
  year = {2013},
  link = {http://arxiv.org/abs/1306.2365}
}
</pre>

</span>
</li>
<li><span id="Schober">Schober, M., Kasenburg, N., Feragen, A., Hennig, P., &amp; Hauberg, S. (2014). Probabilistic shortest path tractography in DTI using
                  Gaussian Process ODE solvers. In <i>Medical Image Computing and Computer-Assisted
                  Intervention–MICCAI 2014</i>. Springer.</span>

<span id="Schober_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#Schober_abstract" data-toggle="collapse" href="#Schober" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#Schober_bibtex" data-toggle="collapse" href="#Schober" onclick="return false">Bib</a></li>

    
    <li><a class="bib-materials" href="../assets/pdf/Schober2014MICCAI.pdf">PDF</a></li>
    
    
    

    
    <li><a class="bib-materials" href="https://www.youtube.com/watch?v=VrhulgVaRMg">video</a></li>
    
  </ul>

  
  <p id="Schober_abstract" class="collapse">Tractography in diffusion tensor imaging estimates
                  connectivity in the brain through observations of local
                  diffusivity. These observations are noisy and of low
                  resolution and, as a consequence, connections cannot be found
                  with high precision. We use probabilistic numerics to
                  estimate connectivity between regions of interest and
                  contribute a Gaussian Process tractography algorithm which
                  allows for both quantification and visualization of its
                  posterior uncertainty. We use the uncertainty both in
                  visualization of individual tracts as well as in heat maps of
                  tract locations.  Finally, we provide a quantitative
                  evaluation of different metrics and algorithms showing that
                  the adjoint metric combined with our algorithm produces paths
                  which agree most often with experts.</p>
  

  <pre id="Schober_bibtex" class="pre pre-scrollable collapse">@inproceedings{Schober,
  author = {Schober, Michael and Kasenburg, Niklas and Feragen, Aasa and Hennig, Philipp and Hauberg, S{\o}ren},
  booktitle = {{Medical Image Computing and Computer-Assisted
                    Intervention--MICCAI 2014}},
  publisher = {Springer},
  title = {{Probabilistic shortest path tractography in {DTI} using
                    {G}aussian {P}rocess {ODE} solvers}},
  year = {2014},
  file = {../assets/pdf/Schober2014MICCAI.pdf},
  video = {https://www.youtube.com/watch?v=VrhulgVaRMg}
}
</pre>

</span>
</li>
<li><span id="2014arXiv14062582S">Schober, M., Duvenaud, D., &amp; Hennig, P. (2014). Probabilistic ODE Solvers with Runge-Kutta Means. <i>ArXiv PrePrint 1406.2582</i>.</span>

<span id="2014arXiv14062582S_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#2014arXiv14062582S_abstract" data-toggle="collapse" href="#2014arXiv14062582S" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#2014arXiv14062582S_bibtex" data-toggle="collapse" href="#2014arXiv14062582S" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://arxiv.org/abs/1406.2582">web</a></li>
    

    
  </ul>

  
  <p id="2014arXiv14062582S_abstract" class="collapse">Runge-Kutta methods are the classic family of solvers for
                  ordinary differential equations (ODEs), and the basis for the
                  state-of-the art. Like most numerical methods, they return
                  point estimates. We construct a family of probabilistic
                  numerical methods that instead return a Gauss-Markov process
                  defining a probability distribution over the ODE solution. In
                  contrast to prior work, we construct this family such that
                  posterior means match the outputs of the Runge-Kutta family
                  exactly, thus inheriting their proven good
                  properties. Remaining degrees of freedom not identified by
                  the match to Runge-Kutta are chosen such that the posterior
                  probability measure fits the observed structure of the
                  ODE. Our results shed light on the structure of Runge-Kutta
                  solvers from a new direction, provide a richer, probabilistic
                  output, have low computational cost, and raise new research
                  questions.</p>
  

  <pre id="2014arXiv14062582S_bibtex" class="pre pre-scrollable collapse">@article{2014arXiv14062582S,
  author = {Schober, M. and Duvenaud, D. and Hennig, P.},
  title = {Probabilistic ODE Solvers with {R}unge-{K}utta Means},
  journal = {arXiv prePrint 1406.2582},
  year = {2014},
  month = {jun},
  link = {http://arxiv.org/abs/1406.2582}
}
</pre>

</span>
</li>
<li><span id="2014arXiv14083807B">Barber, D. (2014). On solving Ordinary Differential Equations using Gaussian
                  Processes. <i>ArXiv Pre-Print 1408.3807</i>.</span>

<span id="2014arXiv14083807B_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#2014arXiv14083807B_abstract" data-toggle="collapse" href="#2014arXiv14083807B" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#2014arXiv14083807B_bibtex" data-toggle="collapse" href="#2014arXiv14083807B" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://arxiv.org/abs/1408.3807">web</a></li>
    

    
  </ul>

  
  <p id="2014arXiv14083807B_abstract" class="collapse">We describe a set of Gaussian Process based approaches that
                  can be used to solve non-linear Ordinary Differential
                  Equations. We suggest an explicit probabilistic solver and
                  two implicit methods, one analogous to Picard iteration and
                  the other to gradient matching. All methods have greater
                  accuracy than previously suggested Gaussian Process
                  approaches. We also suggest a general approach that can yield
                  error estimates from any standard ODE solver.</p>
  

  <pre id="2014arXiv14083807B_bibtex" class="pre pre-scrollable collapse">@article{2014arXiv14083807B,
  author = {{Barber}, D.},
  title = {{On solving Ordinary Differential Equations using Gaussian
                    Processes}},
  journal = {ArXiv pre-print 1408.3807},
  year = {2014},
  month = {aug},
  link = {http://arxiv.org/abs/1408.3807}
}
</pre>

</span>
</li></ol>

<h2 id="PDEs">Partial Differential Equations</h2>

<ol class="bibliography"><li><span id="2014arXiv14071517B">Bui-Thanh, T., &amp; Girolami, M. (2014). Solving Large-Scale PDE-constrained Bayesian Inverse
                  Problems with Riemann Manifold Hamiltonian Monte Carlo. <i>ArXiv Pre-Print 1407.1517</i>.</span>

<span id="2014arXiv14071517B_materials">
  <ul class="nav nav-pills">
    
    <li><a class="bib-materials" data-target="#2014arXiv14071517B_abstract" data-toggle="collapse" href="#2014arXiv14071517B" onclick="return false">Abstract</a></li>
    

    <li><a class="bib-materials" data-target="#2014arXiv14071517B_bibtex" data-toggle="collapse" href="#2014arXiv14071517B" onclick="return false">Bib</a></li>

    
    
    
    <li><a class="bib-materials" href="http://arxiv.org/abs/1407.1517">web</a></li>
    

    
  </ul>

  
  <p id="2014arXiv14071517B_abstract" class="collapse">We consider the Riemann manifold Hamiltonian Monte Carlo
                  (RMHMC) method for solving statistical inverse problems
                  governed by partial differential equations (PDEs). The power
                  of the RMHMC method is that it exploits the geometric
                  structure induced by the PDE constraints of the underlying
                  inverse problem. Consequently, each RMHMC posterior sample is
                  almost independent from the others providing statistically
                  efficient Markov chain simulation. We reduce the cost of
                  forming the Fisher information matrix by using a low rank
                  approximation via a randomized singular value decomposition
                  technique. This is efficient since a small number of
                  Hessian-vector products are required. The Hessian-vector
                  product in turn requires only two extra PDE solves using the
                  adjoint technique. The results suggest RMHMC as a highly
                  efficient simulation scheme for sampling from PDE induced
                  posterior measures.</p>
  

  <pre id="2014arXiv14071517B_bibtex" class="pre pre-scrollable collapse">@article{2014arXiv14071517B,
  author = {{Bui-Thanh}, T. and {Girolami}, M.},
  title = {{Solving Large-Scale PDE-constrained Bayesian Inverse
                    Problems with Riemann Manifold Hamiltonian Monte Carlo}},
  journal = {arXiv pre-print 1407.1517},
  keywords = {Mathematics - Statistics Theory},
  year = {2014},
  month = {jul},
  link = {http://arxiv.org/abs/1407.1517}
}
</pre>

</span>
</li></ol>

      </div>
      <div class="footer row">
        <p>
    based on code &copy; by <a href="http://github.com/jwvdm/robots-homepage">jwvdm</a> 2013. 
</p>        
      </div>
    </div>
    <script src="https://code.jquery.com/jquery.js"></script>
    <script src="/assets/js/bootstrap.min.js"></script>

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </body>
</html>
