@article{liberty2007randomized,
  title =	 {Randomized algorithms for the low-rank approximation of
                  matrices},
  author =	 {Liberty, Edo and Woolfe, Franco and Martinsson, Per-Gunnar
                  and Rokhlin, Vladimir and Tygert, Mark},
  journal =	 {Proceedings of the National Academy of Sciences},
  volume =	 104,
  number =	 51,
  pages =	 {20167--20172},
  year =	 2007
}

@article{halko2011finding,
  title =	 {Finding structure with randomness: Probabilistic algorithms
                  for constructing approximate matrix decompositions},
  author =	 {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal =	 {SIAM review},
  volume =	 53,
  number =	 2,
  pages =	 {217--288},
  year =	 2011,
  publisher =	 {SIAM}
}

@TechReport{ohagan13-polyn-chaos,
  author =	 {Anthony O'Hagan},
  title =	 {Polynomial Chaos: A Tutorial and Critique from a
                  Statistician's Perspective},
  institution =	 {University of Sheffield, UK},
  year =	 2013,
  month =	 {May}
}

@inproceedings{GarnettOH2013,
  title =	 {Active Learning of Linear Embeddings for Gaussian Processes},
  author =	 {Garnett, R. and Osborne, M. and Hennig, P.},
  booktitle =	 {Proceedings of the 30th Conference on Uncertainty in
                  Artificial Intelligence},
  editor =	 {Zhang, NL and Tian, J},
  publisher =	 {AUAI Press},
  pages =	 {230-239},
  year =	 2014,
  url =		 {http://auai.org/uai2014/proceedings/individuals/152.pdf},
  url2 =	 {https://github.com/rmgarnett/mgp},
  department =	 {Department Sch{\"o}lkopf},
  file =	 {http://auai.org/uai2014/proceedings/individuals/152.pdf},
  code =	 {https://github.com/rmgarnett/mgp},
  abstract = {We propose an active learning method for discovering
                  low-dimensional structure in high-dimensional Gaussian
                  process (GP) tasks. Such problems are increasingly frequent
                  and important, but have hitherto presented severe practical
                  difficulties. We further introduce a novel technique for
                  approximately marginalizing GP hyperparameters, yielding
                  marginal predictions robust to hyperparameter
                  misspecification. Our method offers an efficient means of
                  performing GP regression, quadrature, or Bayesian
                  optimization in high-dimensional spaces.}
}

@article{HennigS2012,
  title =	 {Entropy Search for Information-Efficient Global Optimization},
  author =	 {Hennig, P. and Schuler, CJ.},
  month =	 jun,
  volume =	 13,
  pages =	 {1809-1837},
  abstract =	 {Contemporary global optimization algorithms are based on
                  local measures of utility, rather than a probability measure
                  over location and value of the optimum. They thus attempt to
                  collect low function values, not to learn about the
                  optimum. The reason for the absence of probabilistic global
                  optimizers is that the corresponding inference problem is
                  intractable in several ways. This paper develops desiderata
                  for probabilistic optimization algorithms, then presents a
                  concrete algorithm which addresses each of the computational
                  intractabilities with a sequence of approximations and
                  explicitly adresses the decision problem of maximizing
                  information gain from each evaluation. },
  journal =	 {Journal of Machine Learning Research},
  year =	 2012,
  file =
                  {http://jmlr.csail.mit.edu/papers/volume13/hennig12a/hennig12a.pdf},
  link =	 {http://jmlr.csail.mit.edu/papers/v13/hennig12a.html},
  code = {http://probabilistic-optimization.org/Global.html}
}