@article{1987,
 jstor_articletype = {research-article},
 title = {Monte Carlo is Fundamentally Unsound},
 author = {O'Hagan, A.},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 jstor_issuetitle = {Special Issue: Practical Bayesian Statistics},
 volume = {36},
 number = {2/3},
 jstor_formatteddate = {1987},
 pages = {pp. 247-249},
 link = {http://www.jstor.org/stable/2348519},
 ISSN = {00390526},
 abstract = {We present some fundamental objections to the Monte Carlo method of numerical integration.},
 language = {English},
 year = {1987},
 publisher = {Wiley for the Royal Statistical Society},
 copyright = {Copyright © 1987 Royal Statistical Society}
}

@article{kennedy1996iterative,
  title={Iterative rescaling for Bayesian quadrature},
  author={Kennedy, MC and O’Hagan, A},
  journal={Bayesian Statistics},
  volume={5},
  pages={639--645},
  year={1996},
  publisher={Oxford University Press Oxford}
}

@article{kennedy1998bayesian,
  title={Bayesian quadrature with non-normal approximating functions},
  author={Kennedy, Marc},
  journal={Statistics and Computing},
  volume={8},
  number={4},
  pages={365--375},
  year={1998},
  link={http://dl.acm.org/citation.cfm?id=599295},
  publisher={Springer}
}


@article{o1991bayes,
  title =	 {{B}ayes--{H}ermite quadrature},
  author =	 {O'Hagan, A.},
  journal =	 {Journal of statistical planning and inference},
  volume =	 29,
  number =	 3,
  pages =	 {245--260},
  year =	 1991,
  abstract =	 {Bayesian quadrature treats the problem of numerical
                  integration as one of statistical inference. A prior Gaussian
                  process distribution is assumed for the integrand,
                  observations arise from evaluating the integrand at selected
                  points, and a posterior distribution is derived for the
                  integrand and the integral. Methods are developed for
                  quadrature in p. A particular application is integrating the
                  posterior density arising from some other Bayesian analysis.
                  Simulation results are presented, to show that the resulting
                  Bayes–Hermite quadrature rules may perform better than the
                  conventional Gauss–Hermite rules for this application. A key
                  result is derived for product designs, which makes Bayesian
                  quadrature practically useful for integrating in several
                  dimensions. Although the method does not at present provide a
                  solution to the more difficult problem of quadrature in high
                  dimensions, it does seem to offer real improvements over
                  existing methods in relatively low dimensions.}
}

@techreport{minka2000deriving,
  author =	 {T.P. Minka},
  institution =	 {Statistics Department, Carnegie Mellon University},
  title =	 {{Deriving quadrature rules from {G}aussian processes}},
  year =	 2000,
  link = {http://research.microsoft.com/en-us/um/people/minka/papers/quadrature.html},
  abstract =	 {Quadrature rules are often designed to achieve zero error on
                  a small set of functions, e.g. polynomials of specified
                  degree. A more robust method is to minimize average error
                  over a large class or distribution of functions. If functions
                  are distributed according to a Gaussian process, then
                  designing an average-case quadrature rule reduces to solving
                  a system of 2n equations, where n is the number of nodes in
                  the rule (O'Hagan, 1991). It is shown how this very general
                  technique can be used to design customized quadrature rules,
                  in the style of Yarvin & Rokhlin (1998), without the need for
                  singular value decomposition and in any number of
                  dimensions. It is also shown how classical Gaussian
                  quadrature rules, trigonometric lattice rules, and spline
                  rules can be extended to the average-case and to multiple
                  dimensions by deriving them from Gaussian processes. In
                  addition to being more robust, multidimensional quadrature
                  rules designed for the average-case are found to be much less
                  ambiguous than those designed for a given polynomial degree.}
}

@inproceedings{ghahramani2002bayesian,
  title={Bayesian {Monte Carlo}},
  author={Ghahramani, Zoubin and Rasmussen, Carl E},
  booktitle={Advances in neural information processing systems},
  pages={489--496},
  file={http://machinelearning.wustl.edu/mlpapers/paper_files/AA01.pdf},
  year={2002},
  abstract = { We investigate Bayesian alternatives to classical Monte Carlo
    methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the in-
    corporation of prior knowledge, such as smoothness of the integrand, into
    the estimation. In a simple problem we show that this outperforms any
    classical importance sampling method. We also attempt more chal- lenging
    multidimensional integrals involved in computing marginal like- lihoods of
    statistical models (a.k.a. partition functions and model evi- dences). We
    find that Bayesian Monte Carlo outperformed Annealed Importance Sampling,
    although for very high dimensional problems or problems with massive
    multimodality BMC may be less adequate. One advantage of the Bayesian
    approach to Monte Carlo is that samples can be drawn from any distribution.
    This allows for the possibility of active design of sample points so as to
    maximise information gain. }
}


@inproceedings{osborne2012bayesian,
  author =	 {M.A. Osborne and R. Garnett and S.J. Roberts and C. Hart and
                  S.  Aigrain and N. Gibson},
  booktitle =	 {{International Conference on Artificial Intelligence and
                  Statistics}},
  pages =	 {832--840},
  title =	 {{Bayesian quadrature for ratios}},
  year =	 2012,
  abstract =	 {We describe a novel approach to quadrature for ratios of
                  probabilistic integrals, such as are used to compute
                  posterior probabilities. This approach offers performance
                  superior to Monte Carlo methods by exploiting a Bayesian
                  quadrature framework. We improve upon previous Bayesian
                  quadrature techniques by explicitly modelling the
                  non-negativity of our integrands, and the correlations that
                  exist between them. It offers most where the integrand is
                  multi-modal and expensive to evaluate. We demonstrate the
                  efficacy of our method on data from the Kepler space
                  telescope.},
  file =	 {../assets/pdf/Osborne2012Bayesian.pdf}
}


@inproceedings{osborne2012active,
  author =	 {M.A. Osborne and D.K. Duvenaud and R. Garnett and
                  C.E. Rasmussen and S.J. Roberts and Z. Ghahramani},
  booktitle =	 {{Advances in Neural Information Processing Systems (NIPS)}},
  pages =	 {46--54},
  title =	 {{Active Learning of Model Evidence Using Bayesian
                  Quadrature.}},
  year =	 2012,
  abstract =	 { Numerical integration is a key component of many problems in
                  scientific computing, statistical modelling, and machine
                  learning. Bayesian Quadrature is a model-based method for
                  numerical integration which, relative to standard Monte Carlo
                  methods, offers increased sample efficiency and a more robust
                  estimate of the uncertainty in the estimated integral. We
                  propose a novel Bayesian Quadrature approach for numerical
                  integration when the integrand is non-negative, such as the
                  case of computing the marginal likelihood, predictive
                  distribution, or normalising constant of a probabilistic
                  model. Our approach approximately marginalises the quadrature
                  model’s hyperparameters in closed form, and introduces an ac-
                  tive learning scheme to optimally select function
                  evaluations, as opposed to using Monte Carlo samples. We
                  demonstrate our method on both a number of synthetic
                  benchmarks and a real scientific problem from astronomy.},
 file =	 {../assets/pdf/Osborne2012active.pdf
}
}

@inproceedings{sarkkagaussian,
  title =	 {Gaussian Process Quadratures in Nonlinear Sigma-Point
                  Filtering and Smoothing},
  author =	 {S{\"a}rkk{\"a}, Simo and Hartikainen, Jouni and Svensson,
                  Lennart and Sandblom, Fredrik},
  booktitle =	 {FUSION},
  year =	 2014,
  file =	 {http://becs.aalto.fi/~ssarkka/pub/gp_quad_fusion_2014.pdf},
  abstract =	 {This paper is concerned with the use of Gaussian process
                  regression based quadrature rules in the context of sigma-
                  point-based nonlinear Kalman filtering and smoothing. We show
                  how Gaussian process (i.e., Bayesian or Bayes–Hermite)
                  quadratures can be used for numerical solving of the
                  Gaussian integrals arising in the filters and smoothers. An
                  interesting additional result is that with suitable
                  selections of Hermite polynomial covariance functions the
                  Gaussian process quadratures can be reduced to unscented
                  transforms, spherical cubature rules, and to Gauss- Hermite
                  rules previously proposed for approximate nonlinear Kalman
                  filter and smoothing. Finally, the performance of the
                  Gaussian process quadratures in this context is evaluated
                  with numerical simulations. }
}

@InProceedings{gunter14-fast-bayesian-quadrature,
  author =	 {Tom Gunter and Michael A. Osborne and Roman Garnett and
                  Philipp Hennig and Stephen Roberts},
  title =	 {Sampling for Inference in Probabilistic Models with Fast
                  Bayesian Quadrature},
  booktitle =	 {Advances in Neural Information Processing Systems (NIPS)},
  year =	 2014,
  editor =	 {C. Cortes and N. Lawrence},
  abstract =	 {We propose a novel sampling framework for inference in
                  probabilistic models: an active learning approach that
                  converges more quickly (in wall-clock time) than Markov chain
                  Monte Carlo (MCMC) benchmarks. The central challenge in
                  probabilistic inference is numerical integration, to average
                  over ensembles of models or unknown (hyper-)parameters (for
                  example to compute marginal likelihood or a partition
                  function). MCMC has provided approaches to numerical
                  integration that deliver state-of-the-art inference, but can
                  suffer from sample inefficiency and poor convergence
                  diagnostics. Bayesian quadrature techniques offer a
                  model-based solution to such problems, but their uptake has
                  been hindered by prohibitive computation costs. We introduce
                  a warped model for probabilistic integrands (likelihoods)
                  that are known to be non-negative, permitting a cheap active
                  learning scheme to optimally select sample locations. Our
                  algorithm is demonstrated to offer faster convergence (in
                  seconds) relative to simple Monte Carlo and annealed
                  importance sampling on both synthetic and real-world
                  examples.},
  code =	 {https://github.com/OxfordML/wsabi}
}


@Article{oates14-contr-monte-carlo,
  author =	 {Chris J. Oates and Mark Girolami and Nicolas Chopin},
  title =	 {Control functionals for Monte Carlo integration},
  journal =	 {arXiv preprint 1410.2392},
  year =	 2014,
  abstract =	 {This paper introduces a novel class of estimators for Monte
                  Carlo integration, that leverage gradient information in
                  order to provide the improved estimator performance demanded
                  by contemporary statistical applications. The proposed
                  estimators, called "control functionals", achieve sub-root-n
                  convergence and often require orders of magnitude fewer
                  simulations, compared with existing approaches, in order to
                  achieve a fixed level of precision. We focus on a particular
                  sub-class of estimators that permit an elegant analytic form
                  and study their properties, both theoretically and
                  empirically. Results are presented on Bayes-Hermite
                  quadrature, hierarchical Gaussian process models and
                  non-linear ordinary differential equation models, where in
                  each case our estimators are shown to offer state of the art
                  performance.},
  link =          {http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/oates/control_functionals},
  file =	 {http://arxiv.org/pdf/1410.2392v1}
}

@ARTICLE{2015arXiv150405994S,
   author = {{S{\"a}rkk{\"a}}, S. and {Hartikainen}, J. and {Svensson}, L. and
  {Sandblom}, F.},
    title = "{On the relation between Gaussian process quadratures and sigma-point methods}",
  journal = {arXiv preprint stat.ME 1504.05994},
     year = 2015,
    month = apr,
    abstract = {This article is concerned with Gaussian process quadratures, which are numerical integration methods based on Gaussian process regression methods, and sigma-point methods, which are used in advanced non-linear Kalman filtering and smoothing algorithms. We show that many sigma-point methods can be interpreted as Gaussian quadrature based methods with suitably selected covariance functions. We show that this interpretation also extends to more general multivariate Gauss--Hermite integration methods and related spherical cubature rules. Additionally, we discuss different criteria for selecting the sigma-point locations: exactness for multivariate polynomials up to a given order, minimum average error, and quasi-random point sets. The performance of the different methods is tested in numerical experiments.},
    file = {http://arxiv.org/pdf/1504.05994v1.pdf}
}




